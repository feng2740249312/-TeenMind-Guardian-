"""
BERTæƒ…æ„Ÿåˆ†ææ¨¡å—ï¼ˆæ ¸å¿ƒAIæ¨¡å—â­â­â­â­â­ï¼‰
"""

import torch
import torch.nn as nn
from transformers import BertTokenizer, BertForSequenceClassification
from typing import List, Dict
import numpy as np
import json
import os
from datetime import datetime

class EmotionAnalyzer:
    def __init__(self, model_path: str = None):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model_name = model_path or 'hfl/chinese-bert-wwm-ext'
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.emotion_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=6).to(self.device)
        self.slang_dict = self._load_slang_dict()
        self.emotion_labels = {0: 'positive',1: 'negative',2: 'neutral',3: 'depression',4: 'anxiety',5: 'suicidal'}
        self.risk_weights = {'positive': -10,'negative': 10,'neutral': 0,'depression': 30,'anxiety': 25,'suicidal': 50}

    def analyze(self, text: str) -> Dict:
        processed_text = self._preprocess_text(text)
        inputs = self.tokenizer(processed_text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(self.device)
        with torch.no_grad():
            outputs = self.emotion_model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=-1)[0]
        emotions = {label: float(probs[idx]) for idx, label in self.emotion_labels.items()}
        primary_emotion_idx = torch.argmax(probs).item()
        primary_emotion = self.emotion_labels[primary_emotion_idx]
        confidence = float(probs[primary_emotion_idx])
        risk_score = self._calculate_risk_score(emotions)
        keywords = self._extract_keywords(text)
        suggestion = self._generate_suggestion(primary_emotion, risk_score)
        return {
            'text': text,
            'processed_text': processed_text,
            'primary_emotion': primary_emotion,
            'confidence': confidence,
            'emotions': emotions,
            'risk_score': risk_score,
            'risk_level': self._get_risk_level(risk_score),
            'keywords': keywords,
            'suggestion': suggestion,
            'timestamp': datetime.now().isoformat()
        }

    def batch_analyze(self, texts: List[str]) -> List[Dict]:
        return [self.analyze(t) for t in texts]

    def _preprocess_text(self, text: str) -> str:
        processed = text
        for slang, standard in self.slang_dict.items():
            processed = processed.replace(slang, standard)
        import re
        processed = re.sub(r'[!ï¼]{2,}', 'ï¼', processed)
        processed = re.sub(r'[?ï¼Ÿ]{2,}', 'ï¼Ÿ', processed)
        processed = re.sub(r'[.ã€‚]{2,}', 'ã€‚', processed)
        return processed

    def _calculate_risk_score(self, emotions: Dict[str, float]) -> float:
        score = 0.0
        for emotion, prob in emotions.items():
            score += prob * self.risk_weights.get(emotion, 0)
        score = max(0, min(100, score))
        return round(score, 2)

    def _get_risk_level(self, risk_score: float) -> str:
        if risk_score < 30:
            return 'ğŸŸ¢ ä½é£é™©'
        elif risk_score < 70:
            return 'ğŸŸ¡ ä¸­é£é™©'
        else:
            return 'ğŸ”´ é«˜é£é™©'

    def _extract_keywords(self, text: str) -> List[str]:
        high_risk_keywords = ['è‡ªæ€','æƒ³æ­»','ä¸æƒ³æ´»','ç»“æŸç”Ÿå‘½','è§£è„±','æŠ‘éƒ','ç„¦è™‘','å´©æºƒ','ç»æœ›','ç—›è‹¦','å¤±çœ ','å­¤ç‹¬']
        return [k for k in high_risk_keywords if k in text]

    def _generate_suggestion(self, emotion: str, risk_score: float) -> str:
        if risk_score < 30:
            return 'çŠ¶æ€ç¨³å®šï¼Œç»§ç»­ä¿æŒç§¯æå¿ƒæ€ã€‚'
        elif risk_score < 70:
            return 'å‡ºç°ä¸€äº›è´Ÿé¢æƒ…ç»ªï¼Œå»ºè®®ä¸ä¿¡ä»»çš„äººæ²Ÿé€šå¹¶ä¿æŒè§„å¾‹ä½œæ¯ã€‚'
        else:
            return 'é«˜é£é™©è­¦ç¤ºï¼šå»ºè®®ç«‹å³å¯»æ±‚å®¶äººã€æœ‹å‹é™ªä¼´å¹¶è”ç³»ä¸“ä¸šå¿ƒç†å’¨è¯¢å¸ˆã€‚'

    def _load_slang_dict(self) -> Dict[str, str]:
        slang_file = 'data/teen_slang.json'
        if os.path.exists(slang_file):
            try:
                with open(slang_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    mapping = {}
                    for section in data.values():
                        if isinstance(section, dict):
                            for k,v in section.get('standard_mapping', {}).items():
                                mapping[k] = v
                    return mapping
            except Exception:
                pass
        return {'emoäº†': 'æƒ…ç»ªä½è½','ç ´é˜²äº†': 'å¿ƒç†é˜²çº¿å´©æºƒ','éº»äº†': 'éº»æœ¨','æ‘†çƒ‚': 'è‡ªæš´è‡ªå¼ƒ','èººå¹³': 'æ”¾å¼ƒåŠªåŠ›'}
